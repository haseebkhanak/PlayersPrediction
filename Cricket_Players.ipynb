{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKSoR0s8MUoQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82NTbVUJMZNP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b96dedc9-dfda-4b97-9eca-1457f552ac0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Id         Player  Country  Debut  Total Matches  Not Out  Total Runs  \\\n",
              "0    0    Babar Azam       Pak   2016            273       36       13112   \n",
              "1    1    Jos Buttler      Eng   2015            352       57       10856   \n",
              "2    2    Virat Kohli      Ind   2010            520       86       26704   \n",
              "3    4       Bairstow      Eng   2014            272       33       11184   \n",
              "4    5         Rizwan      Pak   2016            189       39        6501   \n",
              "5    6          Smith      Aus   2010            325       50       15949   \n",
              "6    7           Root      Eng   2010            338       48       18831   \n",
              "7    8         Warner      Aus   2012            372       25       18612   \n",
              "8    9     Williamson       Nz   2012            348       44       17537   \n",
              "9   10          Rohit      Ind   2010            464       63       18299   \n",
              "10  11         Dhawan      Ind   2013            269       14       10867   \n",
              "11  14   Kusal Perera       Sr   2015            201       10        6074   \n",
              "12  15         Marnus      Aus   2016             96        9        5628   \n",
              "13  16    Travis Head      Aus   2017            132       13        5932   \n",
              "14  17         Conway       Nz   2018             91       12        3944   \n",
              "15  18         Shakib      Ban   2010            430       54       14406   \n",
              "16  19   Fakhar Zaman      Pak   2017            161        9        5117   \n",
              "17  20     Alex Hales      Eng   2016            156       11        5066   \n",
              "18  21      Shai Hope       WI   2017            185       21        7351   \n",
              "19  22           Imam      Pak   2017             98       10        4727   \n",
              "20  23          Marsh      Aus   2011            176       27        5798   \n",
              "21  24  Ahmed Shehzad      Pak   2011            153        4        5058   \n",
              "22  25      Jason Roy      Eng   2014            185        4        5980   \n",
              "23  26         Morgan      Eng   2010            379       56       10859   \n",
              "24  27          Finch      Aus   2012            254       15        8804   \n",
              "25  28           Wade      Aus   2017            213       40        4626   \n",
              "26  29   Rishabh Pant      Ind   2017            129       17        4123   \n",
              "27  30         Rahane      Ind   2011            195       17        8414   \n",
              "28  34         Bavuma       SA   2015            131       20        5179   \n",
              "29  35      Liton Das      Ban   2016            202       12        6668   \n",
              "30  36        De Kock       SA   2012            289       22       12347   \n",
              "31  38         Dussen       SA   2018            123       21        4336   \n",
              "32  39     Tom Latham       Nz   2012            249       26        9816   \n",
              "33  40       KL Rahul      Ind   2014            196       24        7840   \n",
              "34  41        Markram       SA   2017            144       14        5637   \n",
              "\n",
              "    Highest Score  Strke Rate  Century  Half Century  Average  \n",
              "0             196       90.67       31            88    48.01  \n",
              "1             162      105.30       14            66    35.44  \n",
              "2             254       95.70       80           139    53.51  \n",
              "3             167       80.16       23            53    36.60  \n",
              "4             131       90.26        6            47    43.20  \n",
              "5             239       88.64       44            77    42.41  \n",
              "6             254       89.94       46           104    44.53  \n",
              "7             335      102.91       49            94    40.92  \n",
              "8             251       85.31       42            95    45.43  \n",
              "9             264       95.86       45           100    42.00  \n",
              "10            190       94.88       24            55    37.54  \n",
              "11            153       98.61        8            24    29.72  \n",
              "12            215       61.96       13            30    30.82  \n",
              "13            175      104.26       11            33    38.38  \n",
              "14            200       56.47        9            20    43.80  \n",
              "15            217       89.06       14            99    33.39  \n",
              "16            210       91.69       11            26    33.42  \n",
              "17            171       92.63        7            31    32.00  \n",
              "18            170       81.42       18            32    32.91  \n",
              "19            157       70.87       12            29    32.03  \n",
              "20            181       94.97        6            34    33.66  \n",
              "21            176       79.26       10            25    33.09  \n",
              "22            180      100.64       12            30    27.58  \n",
              "23            148       94.03       16            64    32.76  \n",
              "24            172       91.74       19            51    33.65  \n",
              "25            117       89.02        5            19    27.81  \n",
              "26            159      102.21        6            19    33.56  \n",
              "27            188       80.47       15            51    31.51  \n",
              "28            172       85.16        7            25    34.22  \n",
              "29            176       92.03        8            38    30.79  \n",
              "30            178      101.63       28            66    39.02  \n",
              "31            134       85.71        6            27    39.04  \n",
              "32            264       80.54       20            54    33.82  \n",
              "33            199       92.94       17            53    40.56  \n",
              "34            175      102.80       10            29    37.14  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0b288273-8912-4376-8a45-0fc24fae54d7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Player</th>\n",
              "      <th>Country</th>\n",
              "      <th>Debut</th>\n",
              "      <th>Total Matches</th>\n",
              "      <th>Not Out</th>\n",
              "      <th>Total Runs</th>\n",
              "      <th>Highest Score</th>\n",
              "      <th>Strke Rate</th>\n",
              "      <th>Century</th>\n",
              "      <th>Half Century</th>\n",
              "      <th>Average</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Babar Azam</td>\n",
              "      <td>Pak</td>\n",
              "      <td>2016</td>\n",
              "      <td>273</td>\n",
              "      <td>36</td>\n",
              "      <td>13112</td>\n",
              "      <td>196</td>\n",
              "      <td>90.67</td>\n",
              "      <td>31</td>\n",
              "      <td>88</td>\n",
              "      <td>48.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Jos Buttler</td>\n",
              "      <td>Eng</td>\n",
              "      <td>2015</td>\n",
              "      <td>352</td>\n",
              "      <td>57</td>\n",
              "      <td>10856</td>\n",
              "      <td>162</td>\n",
              "      <td>105.30</td>\n",
              "      <td>14</td>\n",
              "      <td>66</td>\n",
              "      <td>35.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Virat Kohli</td>\n",
              "      <td>Ind</td>\n",
              "      <td>2010</td>\n",
              "      <td>520</td>\n",
              "      <td>86</td>\n",
              "      <td>26704</td>\n",
              "      <td>254</td>\n",
              "      <td>95.70</td>\n",
              "      <td>80</td>\n",
              "      <td>139</td>\n",
              "      <td>53.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Bairstow</td>\n",
              "      <td>Eng</td>\n",
              "      <td>2014</td>\n",
              "      <td>272</td>\n",
              "      <td>33</td>\n",
              "      <td>11184</td>\n",
              "      <td>167</td>\n",
              "      <td>80.16</td>\n",
              "      <td>23</td>\n",
              "      <td>53</td>\n",
              "      <td>36.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Rizwan</td>\n",
              "      <td>Pak</td>\n",
              "      <td>2016</td>\n",
              "      <td>189</td>\n",
              "      <td>39</td>\n",
              "      <td>6501</td>\n",
              "      <td>131</td>\n",
              "      <td>90.26</td>\n",
              "      <td>6</td>\n",
              "      <td>47</td>\n",
              "      <td>43.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>Smith</td>\n",
              "      <td>Aus</td>\n",
              "      <td>2010</td>\n",
              "      <td>325</td>\n",
              "      <td>50</td>\n",
              "      <td>15949</td>\n",
              "      <td>239</td>\n",
              "      <td>88.64</td>\n",
              "      <td>44</td>\n",
              "      <td>77</td>\n",
              "      <td>42.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>Root</td>\n",
              "      <td>Eng</td>\n",
              "      <td>2010</td>\n",
              "      <td>338</td>\n",
              "      <td>48</td>\n",
              "      <td>18831</td>\n",
              "      <td>254</td>\n",
              "      <td>89.94</td>\n",
              "      <td>46</td>\n",
              "      <td>104</td>\n",
              "      <td>44.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>Warner</td>\n",
              "      <td>Aus</td>\n",
              "      <td>2012</td>\n",
              "      <td>372</td>\n",
              "      <td>25</td>\n",
              "      <td>18612</td>\n",
              "      <td>335</td>\n",
              "      <td>102.91</td>\n",
              "      <td>49</td>\n",
              "      <td>94</td>\n",
              "      <td>40.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>Williamson</td>\n",
              "      <td>Nz</td>\n",
              "      <td>2012</td>\n",
              "      <td>348</td>\n",
              "      <td>44</td>\n",
              "      <td>17537</td>\n",
              "      <td>251</td>\n",
              "      <td>85.31</td>\n",
              "      <td>42</td>\n",
              "      <td>95</td>\n",
              "      <td>45.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>Rohit</td>\n",
              "      <td>Ind</td>\n",
              "      <td>2010</td>\n",
              "      <td>464</td>\n",
              "      <td>63</td>\n",
              "      <td>18299</td>\n",
              "      <td>264</td>\n",
              "      <td>95.86</td>\n",
              "      <td>45</td>\n",
              "      <td>100</td>\n",
              "      <td>42.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>Dhawan</td>\n",
              "      <td>Ind</td>\n",
              "      <td>2013</td>\n",
              "      <td>269</td>\n",
              "      <td>14</td>\n",
              "      <td>10867</td>\n",
              "      <td>190</td>\n",
              "      <td>94.88</td>\n",
              "      <td>24</td>\n",
              "      <td>55</td>\n",
              "      <td>37.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>14</td>\n",
              "      <td>Kusal Perera</td>\n",
              "      <td>Sr</td>\n",
              "      <td>2015</td>\n",
              "      <td>201</td>\n",
              "      <td>10</td>\n",
              "      <td>6074</td>\n",
              "      <td>153</td>\n",
              "      <td>98.61</td>\n",
              "      <td>8</td>\n",
              "      <td>24</td>\n",
              "      <td>29.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>15</td>\n",
              "      <td>Marnus</td>\n",
              "      <td>Aus</td>\n",
              "      <td>2016</td>\n",
              "      <td>96</td>\n",
              "      <td>9</td>\n",
              "      <td>5628</td>\n",
              "      <td>215</td>\n",
              "      <td>61.96</td>\n",
              "      <td>13</td>\n",
              "      <td>30</td>\n",
              "      <td>30.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>16</td>\n",
              "      <td>Travis Head</td>\n",
              "      <td>Aus</td>\n",
              "      <td>2017</td>\n",
              "      <td>132</td>\n",
              "      <td>13</td>\n",
              "      <td>5932</td>\n",
              "      <td>175</td>\n",
              "      <td>104.26</td>\n",
              "      <td>11</td>\n",
              "      <td>33</td>\n",
              "      <td>38.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>17</td>\n",
              "      <td>Conway</td>\n",
              "      <td>Nz</td>\n",
              "      <td>2018</td>\n",
              "      <td>91</td>\n",
              "      <td>12</td>\n",
              "      <td>3944</td>\n",
              "      <td>200</td>\n",
              "      <td>56.47</td>\n",
              "      <td>9</td>\n",
              "      <td>20</td>\n",
              "      <td>43.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>18</td>\n",
              "      <td>Shakib</td>\n",
              "      <td>Ban</td>\n",
              "      <td>2010</td>\n",
              "      <td>430</td>\n",
              "      <td>54</td>\n",
              "      <td>14406</td>\n",
              "      <td>217</td>\n",
              "      <td>89.06</td>\n",
              "      <td>14</td>\n",
              "      <td>99</td>\n",
              "      <td>33.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>19</td>\n",
              "      <td>Fakhar Zaman</td>\n",
              "      <td>Pak</td>\n",
              "      <td>2017</td>\n",
              "      <td>161</td>\n",
              "      <td>9</td>\n",
              "      <td>5117</td>\n",
              "      <td>210</td>\n",
              "      <td>91.69</td>\n",
              "      <td>11</td>\n",
              "      <td>26</td>\n",
              "      <td>33.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>20</td>\n",
              "      <td>Alex Hales</td>\n",
              "      <td>Eng</td>\n",
              "      <td>2016</td>\n",
              "      <td>156</td>\n",
              "      <td>11</td>\n",
              "      <td>5066</td>\n",
              "      <td>171</td>\n",
              "      <td>92.63</td>\n",
              "      <td>7</td>\n",
              "      <td>31</td>\n",
              "      <td>32.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>21</td>\n",
              "      <td>Shai Hope</td>\n",
              "      <td>WI</td>\n",
              "      <td>2017</td>\n",
              "      <td>185</td>\n",
              "      <td>21</td>\n",
              "      <td>7351</td>\n",
              "      <td>170</td>\n",
              "      <td>81.42</td>\n",
              "      <td>18</td>\n",
              "      <td>32</td>\n",
              "      <td>32.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>22</td>\n",
              "      <td>Imam</td>\n",
              "      <td>Pak</td>\n",
              "      <td>2017</td>\n",
              "      <td>98</td>\n",
              "      <td>10</td>\n",
              "      <td>4727</td>\n",
              "      <td>157</td>\n",
              "      <td>70.87</td>\n",
              "      <td>12</td>\n",
              "      <td>29</td>\n",
              "      <td>32.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>23</td>\n",
              "      <td>Marsh</td>\n",
              "      <td>Aus</td>\n",
              "      <td>2011</td>\n",
              "      <td>176</td>\n",
              "      <td>27</td>\n",
              "      <td>5798</td>\n",
              "      <td>181</td>\n",
              "      <td>94.97</td>\n",
              "      <td>6</td>\n",
              "      <td>34</td>\n",
              "      <td>33.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>24</td>\n",
              "      <td>Ahmed Shehzad</td>\n",
              "      <td>Pak</td>\n",
              "      <td>2011</td>\n",
              "      <td>153</td>\n",
              "      <td>4</td>\n",
              "      <td>5058</td>\n",
              "      <td>176</td>\n",
              "      <td>79.26</td>\n",
              "      <td>10</td>\n",
              "      <td>25</td>\n",
              "      <td>33.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>25</td>\n",
              "      <td>Jason Roy</td>\n",
              "      <td>Eng</td>\n",
              "      <td>2014</td>\n",
              "      <td>185</td>\n",
              "      <td>4</td>\n",
              "      <td>5980</td>\n",
              "      <td>180</td>\n",
              "      <td>100.64</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>27.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>26</td>\n",
              "      <td>Morgan</td>\n",
              "      <td>Eng</td>\n",
              "      <td>2010</td>\n",
              "      <td>379</td>\n",
              "      <td>56</td>\n",
              "      <td>10859</td>\n",
              "      <td>148</td>\n",
              "      <td>94.03</td>\n",
              "      <td>16</td>\n",
              "      <td>64</td>\n",
              "      <td>32.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>27</td>\n",
              "      <td>Finch</td>\n",
              "      <td>Aus</td>\n",
              "      <td>2012</td>\n",
              "      <td>254</td>\n",
              "      <td>15</td>\n",
              "      <td>8804</td>\n",
              "      <td>172</td>\n",
              "      <td>91.74</td>\n",
              "      <td>19</td>\n",
              "      <td>51</td>\n",
              "      <td>33.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>28</td>\n",
              "      <td>Wade</td>\n",
              "      <td>Aus</td>\n",
              "      <td>2017</td>\n",
              "      <td>213</td>\n",
              "      <td>40</td>\n",
              "      <td>4626</td>\n",
              "      <td>117</td>\n",
              "      <td>89.02</td>\n",
              "      <td>5</td>\n",
              "      <td>19</td>\n",
              "      <td>27.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>29</td>\n",
              "      <td>Rishabh Pant</td>\n",
              "      <td>Ind</td>\n",
              "      <td>2017</td>\n",
              "      <td>129</td>\n",
              "      <td>17</td>\n",
              "      <td>4123</td>\n",
              "      <td>159</td>\n",
              "      <td>102.21</td>\n",
              "      <td>6</td>\n",
              "      <td>19</td>\n",
              "      <td>33.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>30</td>\n",
              "      <td>Rahane</td>\n",
              "      <td>Ind</td>\n",
              "      <td>2011</td>\n",
              "      <td>195</td>\n",
              "      <td>17</td>\n",
              "      <td>8414</td>\n",
              "      <td>188</td>\n",
              "      <td>80.47</td>\n",
              "      <td>15</td>\n",
              "      <td>51</td>\n",
              "      <td>31.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>34</td>\n",
              "      <td>Bavuma</td>\n",
              "      <td>SA</td>\n",
              "      <td>2015</td>\n",
              "      <td>131</td>\n",
              "      <td>20</td>\n",
              "      <td>5179</td>\n",
              "      <td>172</td>\n",
              "      <td>85.16</td>\n",
              "      <td>7</td>\n",
              "      <td>25</td>\n",
              "      <td>34.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>35</td>\n",
              "      <td>Liton Das</td>\n",
              "      <td>Ban</td>\n",
              "      <td>2016</td>\n",
              "      <td>202</td>\n",
              "      <td>12</td>\n",
              "      <td>6668</td>\n",
              "      <td>176</td>\n",
              "      <td>92.03</td>\n",
              "      <td>8</td>\n",
              "      <td>38</td>\n",
              "      <td>30.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>36</td>\n",
              "      <td>De Kock</td>\n",
              "      <td>SA</td>\n",
              "      <td>2012</td>\n",
              "      <td>289</td>\n",
              "      <td>22</td>\n",
              "      <td>12347</td>\n",
              "      <td>178</td>\n",
              "      <td>101.63</td>\n",
              "      <td>28</td>\n",
              "      <td>66</td>\n",
              "      <td>39.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>38</td>\n",
              "      <td>Dussen</td>\n",
              "      <td>SA</td>\n",
              "      <td>2018</td>\n",
              "      <td>123</td>\n",
              "      <td>21</td>\n",
              "      <td>4336</td>\n",
              "      <td>134</td>\n",
              "      <td>85.71</td>\n",
              "      <td>6</td>\n",
              "      <td>27</td>\n",
              "      <td>39.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>39</td>\n",
              "      <td>Tom Latham</td>\n",
              "      <td>Nz</td>\n",
              "      <td>2012</td>\n",
              "      <td>249</td>\n",
              "      <td>26</td>\n",
              "      <td>9816</td>\n",
              "      <td>264</td>\n",
              "      <td>80.54</td>\n",
              "      <td>20</td>\n",
              "      <td>54</td>\n",
              "      <td>33.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>40</td>\n",
              "      <td>KL Rahul</td>\n",
              "      <td>Ind</td>\n",
              "      <td>2014</td>\n",
              "      <td>196</td>\n",
              "      <td>24</td>\n",
              "      <td>7840</td>\n",
              "      <td>199</td>\n",
              "      <td>92.94</td>\n",
              "      <td>17</td>\n",
              "      <td>53</td>\n",
              "      <td>40.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>41</td>\n",
              "      <td>Markram</td>\n",
              "      <td>SA</td>\n",
              "      <td>2017</td>\n",
              "      <td>144</td>\n",
              "      <td>14</td>\n",
              "      <td>5637</td>\n",
              "      <td>175</td>\n",
              "      <td>102.80</td>\n",
              "      <td>10</td>\n",
              "      <td>29</td>\n",
              "      <td>37.14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b288273-8912-4376-8a45-0fc24fae54d7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0b288273-8912-4376-8a45-0fc24fae54d7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0b288273-8912-4376-8a45-0fc24fae54d7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ba2e1f9e-6e6c-4813-bcc5-da704079ec6d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ba2e1f9e-6e6c-4813-bcc5-da704079ec6d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ba2e1f9e-6e6c-4813-bcc5-da704079ec6d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "dataset=pd.read_csv(\"/content/drive/MyDrive/Players data.csv\")\n",
        "dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhePM7TrMwDy"
      },
      "outputs": [],
      "source": [
        "x=dataset.iloc[:,9:10].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XKqLQrHRr4o"
      },
      "outputs": [],
      "source": [
        "y=dataset.iloc[:,11].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MnrLQIOR97o"
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# LE_x=LabelEncoder()\n",
        "# LE_x.fit(x[:,0])\n",
        "# (x[:,0])=LE_x.transform(x[:,0])\n",
        "# x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnddVINjUJd2"
      },
      "outputs": [],
      "source": [
        "# (x[:,1])=LE_x.fit_transform(x[:,1])\n",
        "# x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mE9Of-hUl9Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import StandardScaler\n",
        "# sc_x=StandardScaler()\n",
        "# x_train=sc_x.fit_transform(x_train)\n",
        "# x_test=sc_x.transform(x_test)"
      ],
      "metadata": {
        "id": "5YrUTNmiSuak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrvutUAjUrpa"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(x_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "linear_pred = regressor.predict([[15]])\n",
        "\n",
        "print(\"Predict the player average who will score 15 centuries\")\n",
        "print(linear_pred)\n",
        "\n",
        "linear_pred = regressor.predict(x_test)\n",
        "plt.scatter(x_train, y_train, color = 'red')\n",
        "plt.plot(x_train, regressor.predict(x_train), color = 'blue')\n",
        "plt.title('Century VS Average')\n",
        "plt.xlabel('Century')\n",
        "plt.ylabel('Average')\n",
        "plt.show()\n",
        "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
        "linear = r2_score(y_test,linear_pred)\n",
        "mse=mean_squared_error(y_test,linear_pred)\n",
        "mae=mean_absolute_error(y_test,linear_pred)\n",
        "print(\"R2 score of linear Regression is \", linear)\n",
        "print(\"R2 score of linear Regression is \", mse)\n",
        "print(\"R2 score of linear Regression is \", mae)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=dataset.iloc[:, [4,5,6,9,10]].values\n",
        "y=dataset.iloc[:, 11].values"
      ],
      "metadata": {
        "id": "WOKafvR5YG6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 1/3, random_state = 0)"
      ],
      "metadata": {
        "id": "5CqcHTBCYHxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_x=StandardScaler()\n",
        "x_train=sc_x.fit_transform(x_train)\n",
        "x_test=sc_x.transform(x_test)"
      ],
      "metadata": {
        "id": "izbuuogiYSgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6Xn9IvvFwtv"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(x_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "# multiple_pred = regressor.predict([[350,11,13000]])\n",
        "\n",
        "# print(\"Average prediction of any individual player who will play 350 matches and score 8000 runs\")\n",
        "# print(multiple_pred)\n",
        "multiple_pred = regressor.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
        "multiple = r2_score(y_test,multiple_pred)\n",
        "mse1=mean_squared_error(y_test,multiple_pred)\n",
        "mae1=mean_absolute_error(y_test,multiple_pred)\n",
        "print(\"R2 score of multiple Regression is \", multiple)\n",
        "print(\"R2 score of multiple Regression is \", mse1)\n",
        "print(\"R2 score of multiple Regression is \", mae1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "SVM=SVR(kernel='linear')\n",
        "SVM.fit(x_train, y_train)\n",
        "SVM_pred=SVM.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
        "svm = r2_score(y_test,SVM_pred)\n",
        "mse2=mean_squared_error(y_test,SVM_pred)\n",
        "mae2=mean_absolute_error(y_test,SVM_pred)\n",
        "print(\"R2 score of SVR(linear) Regressor is \", svm)\n",
        "print(\"R2 score of SVR(linear) Regressor is \", mse2)\n",
        "print(\"R2 score of SVR(linear) Regressor is \", mae2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy_6K3wnWp0T",
        "outputId": "4c460903-63ae-4a36-de15-d2d4846df045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 score of SVR(linear) Regressor is  0.7518322314128241\n",
            "R2 score of SVR(linear) Regressor is  10.708891948375582\n",
            "R2 score of SVR(linear) Regressor is  2.730632148174932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'C': [0.01,0.1,1,10,100], 'gamma': [0.01,0.1,1,10,100]}\n",
        "\n",
        "SVM2 = GridSearchCV(SVR(kernel='rbf'), param_grid)\n",
        "SVM2.fit(x_train, y_train)\n",
        "\n",
        "SVM2_pred = SVM2.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
        "svm2 = r2_score(y_test,SVM2_pred)\n",
        "mse3=mean_squared_error(y_test,SVM2_pred)\n",
        "mae3=mean_absolute_error(y_test,SVM2_pred)\n",
        "print(\"R2 score of SVR(rbf) Regressor is \", svm2)\n",
        "print(\"R2 score of SVR(rbf) Regressor is \", mse3)\n",
        "print(\"R2 score of SVR(rbf) Regressor is \", mae3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ2FU0irXvw0",
        "outputId": "6d8ed358-f685-48b2-edbc-33c57cb653d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 score of SVR(rbf) Regressor is  0.749522912715653\n",
            "R2 score of SVR(rbf) Regressor is  10.808543263061445\n",
            "R2 score of SVR(rbf) Regressor is  2.775660621520719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "RF=RandomForestRegressor(n_estimators=100,random_state=0)\n",
        "RF.fit(x_train, y_train)\n",
        "RF_pred=RF.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
        "rf = r2_score(y_test,RF_pred)\n",
        "mse4=mean_squared_error(y_test,RF_pred)\n",
        "mae4=mean_absolute_error(y_test,RF_pred)\n",
        "print(\"R2 score of Random Forest Regressor is \", rf)\n",
        "print(\"R2 score of Random Forest Regressor is \", mse4)\n",
        "print(\"R2 score of Random Forest Regressor is \", mae4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvmlYxVHYbyN",
        "outputId": "80446c41-3373-40cd-e8e7-676b1d96cb59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 score of Random Forest Regressor is  0.17862733915894136\n",
            "R2 score of Random Forest Regressor is  35.44372875000003\n",
            "R2 score of Random Forest Regressor is  4.989450000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [10, 20, 30, 40],\n",
        "    'min_samples_split': [2, 5, 10, 50],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'splitter': ['best', 'random'],\n",
        "}\n",
        "\n",
        "\n",
        "DT = GridSearchCV(DecisionTreeRegressor(random_state = 0), param_grid)\n",
        "DT.fit(x_train, y_train)\n",
        "DT_pred=DT.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
        "dt = r2_score(y_test,DT_pred)\n",
        "mse5=mean_squared_error(y_test,DT_pred)\n",
        "mae5=mean_absolute_error(y_test,DT_pred)\n",
        "print(\"R2 score of Decision Tree Regressor is \", dt)\n",
        "print(\"R2 score of Decision Tree Regressor is \", mse5)\n",
        "print(\"R2 score of Decision Tree Regressor is \", mae5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IFtQiWXYzbR",
        "outputId": "d549769d-64e5-45bf-ff2c-a4ca0710efe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 score of Decision Tree Regressor is  0.34135781718439917\n",
            "R2 score of Decision Tree Regressor is  28.421611753086406\n",
            "R2 score of Decision Tree Regressor is  4.393777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_neighbors': [1, 3, 5, 7],\n",
        "    'metric': ['euclidean'],\n",
        "    'p': [1, 2],\n",
        "}\n",
        "\n",
        "KN=GridSearchCV(KNeighborsRegressor(), param_grid)\n",
        "KN=KNeighborsRegressor()\n",
        "KN.fit(x_train, y_train)\n",
        "KN_pred=KN.predict(x_test)\n",
        "\n",
        "kn = r2_score(y_test,KN_pred)\n",
        "mse6=mean_squared_error(y_test,KN_pred)\n",
        "mae6=mean_absolute_error(y_test,KN_pred)\n",
        "print(\"R2 score of KNN Regressor is \", kn)\n",
        "print(\"R2 score of KNN Regressor is \", mse6)\n",
        "print(\"R2 score of KNN Regressor is \", mae6)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK7dd7QYZRjv",
        "outputId": "91bd176a-ba8c-48da-ef17-af2aa3be34a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 score of KNN Regressor is  0.32136707687474364\n",
            "R2 score of KNN Regressor is  29.284248666666645\n",
            "R2 score of KNN Regressor is  4.590666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "Regressor=Sequential()\n",
        "\n",
        "Regressor.add(Dense(200, activation='relu', input_dim=5))\n",
        "Regressor.add(Dropout(0.4))\n",
        "Regressor.add(Dense(200, activation='relu'))\n",
        "Regressor.add(Dropout(0.4))\n",
        "Regressor.add(Dense(200, activation='relu'))\n",
        "Regressor.add(Dropout(0.4))\n",
        "Regressor.add(Dense(200, activation='relu'))\n",
        "Regressor.add(Dropout(0.4))\n",
        "Regressor.add(Dense(200, activation='relu'))\n",
        "Regressor.add(Dropout(0.4))\n",
        "Regressor.add(Dense(1, activation='linear'))\n",
        "\n",
        "Regressor.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
        "\n",
        "Regressor.fit(x_train,y_train, batch_size=5, epochs=200)\n",
        "NN_pred=Regressor.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
        "nn = r2_score(y_test, NN_pred)\n",
        "mse7 = mean_squared_error(y_test, NN_pred)\n",
        "mae7 = mean_absolute_error(y_test,NN_pred)\n",
        "\n",
        "print(\"R2 score of Neural Network model is \",nn)\n",
        "print(\"mse of Neural Network model is \",mse7)\n",
        "print(\"mae of Neural Network model is \",mae7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myrY_NyWZrs8",
        "outputId": "b08af26a-3845-42c3-f698-7e8c8e54dfa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "5/5 [==============================] - 1s 4ms/step - loss: 1444.3040 - mse: 1444.3040\n",
            "Epoch 2/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1375.7628 - mse: 1375.7628\n",
            "Epoch 3/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1228.6394 - mse: 1228.6394\n",
            "Epoch 4/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 876.9359 - mse: 876.9359\n",
            "Epoch 5/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 437.8243 - mse: 437.8242\n",
            "Epoch 6/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 284.7937 - mse: 284.7937\n",
            "Epoch 7/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 388.2458 - mse: 388.2458\n",
            "Epoch 8/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 244.5597 - mse: 244.5597\n",
            "Epoch 9/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 261.3188 - mse: 261.3188\n",
            "Epoch 10/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 272.4007 - mse: 272.4007\n",
            "Epoch 11/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 242.4326 - mse: 242.4326\n",
            "Epoch 12/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 181.3068 - mse: 181.3068\n",
            "Epoch 13/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 249.6020 - mse: 249.6020\n",
            "Epoch 14/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 205.5044 - mse: 205.5044\n",
            "Epoch 15/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 147.7780 - mse: 147.7780\n",
            "Epoch 16/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 150.5322 - mse: 150.5322\n",
            "Epoch 17/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 227.9293 - mse: 227.9293\n",
            "Epoch 18/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 178.3550 - mse: 178.3550\n",
            "Epoch 19/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 136.6881 - mse: 136.6881\n",
            "Epoch 20/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 119.5333 - mse: 119.5333\n",
            "Epoch 21/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 206.6852 - mse: 206.6852\n",
            "Epoch 22/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 135.4935 - mse: 135.4935\n",
            "Epoch 23/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 122.3116 - mse: 122.3116\n",
            "Epoch 24/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 132.2439 - mse: 132.2439\n",
            "Epoch 25/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 113.5990 - mse: 113.5990\n",
            "Epoch 26/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 133.9989 - mse: 133.9989\n",
            "Epoch 27/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 111.7357 - mse: 111.7357\n",
            "Epoch 28/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 74.3065 - mse: 74.3065\n",
            "Epoch 29/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 97.0524 - mse: 97.0524\n",
            "Epoch 30/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 112.1871 - mse: 112.1871\n",
            "Epoch 31/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 99.0299 - mse: 99.0299\n",
            "Epoch 32/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 86.7729 - mse: 86.7729\n",
            "Epoch 33/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 68.6694 - mse: 68.6694\n",
            "Epoch 34/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 184.2276 - mse: 184.2276\n",
            "Epoch 35/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 78.8642 - mse: 78.8642\n",
            "Epoch 36/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 118.6334 - mse: 118.6334\n",
            "Epoch 37/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 131.8995 - mse: 131.8995\n",
            "Epoch 38/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 103.3027 - mse: 103.3027\n",
            "Epoch 39/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 131.7967 - mse: 131.7967\n",
            "Epoch 40/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 73.9725 - mse: 73.9725\n",
            "Epoch 41/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 80.6754 - mse: 80.6754\n",
            "Epoch 42/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 89.6571 - mse: 89.6571\n",
            "Epoch 43/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 103.7674 - mse: 103.7674\n",
            "Epoch 44/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 84.5939 - mse: 84.5939\n",
            "Epoch 45/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 102.7151 - mse: 102.7151\n",
            "Epoch 46/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 57.6711 - mse: 57.6711\n",
            "Epoch 47/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 106.4072 - mse: 106.4072\n",
            "Epoch 48/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 74.9155 - mse: 74.9155\n",
            "Epoch 49/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 95.1330 - mse: 95.1330\n",
            "Epoch 50/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 92.0579 - mse: 92.0579\n",
            "Epoch 51/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 50.5880 - mse: 50.5880\n",
            "Epoch 52/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 67.4985 - mse: 67.4985\n",
            "Epoch 53/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 134.3456 - mse: 134.3456\n",
            "Epoch 54/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 113.6301 - mse: 113.6301\n",
            "Epoch 55/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 83.4357 - mse: 83.4357\n",
            "Epoch 56/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 93.1274 - mse: 93.1274\n",
            "Epoch 57/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 56.3111 - mse: 56.3111\n",
            "Epoch 58/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 79.9450 - mse: 79.9450\n",
            "Epoch 59/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 64.0123 - mse: 64.0123\n",
            "Epoch 60/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 103.2578 - mse: 103.2578\n",
            "Epoch 61/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 63.5305 - mse: 63.5305\n",
            "Epoch 62/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 77.3281 - mse: 77.3281\n",
            "Epoch 63/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 54.8389 - mse: 54.8389\n",
            "Epoch 64/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 43.4685 - mse: 43.4685\n",
            "Epoch 65/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 56.9506 - mse: 56.9506\n",
            "Epoch 66/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 144.4515 - mse: 144.4515\n",
            "Epoch 67/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 65.8941 - mse: 65.8941\n",
            "Epoch 68/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 89.1057 - mse: 89.1057\n",
            "Epoch 69/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 26.0916 - mse: 26.0916\n",
            "Epoch 70/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 69.6341 - mse: 69.6341\n",
            "Epoch 71/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 114.9540 - mse: 114.9540\n",
            "Epoch 72/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 74.4358 - mse: 74.4358\n",
            "Epoch 73/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 79.8345 - mse: 79.8345\n",
            "Epoch 74/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 115.0161 - mse: 115.0161\n",
            "Epoch 75/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 52.9373 - mse: 52.9373\n",
            "Epoch 76/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 71.7827 - mse: 71.7827\n",
            "Epoch 77/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 55.3688 - mse: 55.3688\n",
            "Epoch 78/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 123.6512 - mse: 123.6512\n",
            "Epoch 79/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 57.5289 - mse: 57.5289\n",
            "Epoch 80/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 42.5119 - mse: 42.5119\n",
            "Epoch 81/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 46.8194 - mse: 46.8194\n",
            "Epoch 82/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 33.9232 - mse: 33.9232\n",
            "Epoch 83/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 65.9531 - mse: 65.9531\n",
            "Epoch 84/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 73.8197 - mse: 73.8197\n",
            "Epoch 85/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 64.7294 - mse: 64.7294\n",
            "Epoch 86/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 48.2987 - mse: 48.2987\n",
            "Epoch 87/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 48.2724 - mse: 48.2724\n",
            "Epoch 88/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 32.6284 - mse: 32.6284\n",
            "Epoch 89/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 48.2304 - mse: 48.2304\n",
            "Epoch 90/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 54.8189 - mse: 54.8189\n",
            "Epoch 91/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 40.5731 - mse: 40.5731\n",
            "Epoch 92/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 45.3264 - mse: 45.3264\n",
            "Epoch 93/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 88.7393 - mse: 88.7393\n",
            "Epoch 94/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 35.8680 - mse: 35.8680\n",
            "Epoch 95/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 40.0226 - mse: 40.0226\n",
            "Epoch 96/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 62.9176 - mse: 62.9176\n",
            "Epoch 97/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 37.9090 - mse: 37.9090\n",
            "Epoch 98/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 74.6909 - mse: 74.6909\n",
            "Epoch 99/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 61.8957 - mse: 61.8957\n",
            "Epoch 100/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 70.9759 - mse: 70.9759\n",
            "Epoch 101/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 59.2291 - mse: 59.2291\n",
            "Epoch 102/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 32.8841 - mse: 32.8841\n",
            "Epoch 103/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 54.5702 - mse: 54.5702\n",
            "Epoch 104/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 74.8336 - mse: 74.8336\n",
            "Epoch 105/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 65.6165 - mse: 65.6165\n",
            "Epoch 106/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 70.8607 - mse: 70.8607\n",
            "Epoch 107/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 32.6100 - mse: 32.6100\n",
            "Epoch 108/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 66.5949 - mse: 66.5949\n",
            "Epoch 109/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 37.6906 - mse: 37.6906\n",
            "Epoch 110/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 48.5788 - mse: 48.5788\n",
            "Epoch 111/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 59.5520 - mse: 59.5520\n",
            "Epoch 112/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 82.1560 - mse: 82.1560\n",
            "Epoch 113/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 71.5513 - mse: 71.5513\n",
            "Epoch 114/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 79.2083 - mse: 79.2083\n",
            "Epoch 115/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 36.7883 - mse: 36.7883\n",
            "Epoch 116/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 91.5822 - mse: 91.5822\n",
            "Epoch 117/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 62.4027 - mse: 62.4027\n",
            "Epoch 118/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 53.1219 - mse: 53.1219\n",
            "Epoch 119/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 57.3299 - mse: 57.3299\n",
            "Epoch 120/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 69.0039 - mse: 69.0039\n",
            "Epoch 121/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 53.8945 - mse: 53.8945\n",
            "Epoch 122/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 91.0458 - mse: 91.0458\n",
            "Epoch 123/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 64.8423 - mse: 64.8423\n",
            "Epoch 124/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 37.3700 - mse: 37.3700\n",
            "Epoch 125/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 57.4481 - mse: 57.4481\n",
            "Epoch 126/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 30.4232 - mse: 30.4232\n",
            "Epoch 127/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 30.7372 - mse: 30.7372\n",
            "Epoch 128/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 78.6721 - mse: 78.6721\n",
            "Epoch 129/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 66.7184 - mse: 66.7184\n",
            "Epoch 130/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 43.3424 - mse: 43.3424\n",
            "Epoch 131/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 53.0823 - mse: 53.0823\n",
            "Epoch 132/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 71.8626 - mse: 71.8626\n",
            "Epoch 133/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 43.3546 - mse: 43.3546\n",
            "Epoch 134/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 81.3278 - mse: 81.3278\n",
            "Epoch 135/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 63.4566 - mse: 63.4566\n",
            "Epoch 136/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 72.2948 - mse: 72.2948\n",
            "Epoch 137/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 47.0117 - mse: 47.0117\n",
            "Epoch 138/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 67.9407 - mse: 67.9407\n",
            "Epoch 139/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 33.2463 - mse: 33.2463\n",
            "Epoch 140/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 53.8733 - mse: 53.8733\n",
            "Epoch 141/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 68.4542 - mse: 68.4542\n",
            "Epoch 142/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 30.8142 - mse: 30.8142\n",
            "Epoch 143/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 66.5885 - mse: 66.5885\n",
            "Epoch 144/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 30.7626 - mse: 30.7626\n",
            "Epoch 145/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 62.5742 - mse: 62.5742\n",
            "Epoch 146/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 40.7763 - mse: 40.7763\n",
            "Epoch 147/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 48.7953 - mse: 48.7953\n",
            "Epoch 148/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 28.7683 - mse: 28.7683\n",
            "Epoch 149/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 123.7541 - mse: 123.7541\n",
            "Epoch 150/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 75.3261 - mse: 75.3261\n",
            "Epoch 151/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 62.0304 - mse: 62.0304\n",
            "Epoch 152/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 71.1230 - mse: 71.1230\n",
            "Epoch 153/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 43.4876 - mse: 43.4876\n",
            "Epoch 154/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 63.2864 - mse: 63.2864\n",
            "Epoch 155/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 66.1447 - mse: 66.1447\n",
            "Epoch 156/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 50.8629 - mse: 50.8629\n",
            "Epoch 157/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 75.0007 - mse: 75.0007\n",
            "Epoch 158/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 41.2982 - mse: 41.2982\n",
            "Epoch 159/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 42.7714 - mse: 42.7714\n",
            "Epoch 160/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 19.2067 - mse: 19.2067\n",
            "Epoch 161/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 49.8925 - mse: 49.8925\n",
            "Epoch 162/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 33.2801 - mse: 33.2801\n",
            "Epoch 163/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 52.7834 - mse: 52.7834\n",
            "Epoch 164/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 79.1154 - mse: 79.1154\n",
            "Epoch 165/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 41.8422 - mse: 41.8422\n",
            "Epoch 166/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 57.5586 - mse: 57.5586\n",
            "Epoch 167/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 67.4502 - mse: 67.4502\n",
            "Epoch 168/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 44.6956 - mse: 44.6956\n",
            "Epoch 169/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 62.5067 - mse: 62.5067\n",
            "Epoch 170/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 53.9620 - mse: 53.9620\n",
            "Epoch 171/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 48.7887 - mse: 48.7887\n",
            "Epoch 172/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 35.1337 - mse: 35.1337\n",
            "Epoch 173/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 41.0425 - mse: 41.0425\n",
            "Epoch 174/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 41.4663 - mse: 41.4663\n",
            "Epoch 175/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 33.8696 - mse: 33.8696\n",
            "Epoch 176/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 27.1874 - mse: 27.1874\n",
            "Epoch 177/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 32.1516 - mse: 32.1516\n",
            "Epoch 178/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 57.4789 - mse: 57.4789\n",
            "Epoch 179/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 92.8615 - mse: 92.8615\n",
            "Epoch 180/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 51.4560 - mse: 51.4560\n",
            "Epoch 181/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 65.6299 - mse: 65.6299\n",
            "Epoch 182/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 44.2478 - mse: 44.2478\n",
            "Epoch 183/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 65.7220 - mse: 65.7220\n",
            "Epoch 184/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 39.8508 - mse: 39.8508\n",
            "Epoch 185/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 22.2917 - mse: 22.2917\n",
            "Epoch 186/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 42.1918 - mse: 42.1918\n",
            "Epoch 187/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 56.1293 - mse: 56.1293\n",
            "Epoch 188/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 36.9565 - mse: 36.9565\n",
            "Epoch 189/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 29.8006 - mse: 29.8006\n",
            "Epoch 190/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 60.7046 - mse: 60.7046\n",
            "Epoch 191/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 32.0681 - mse: 32.0681\n",
            "Epoch 192/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 58.6425 - mse: 58.6425\n",
            "Epoch 193/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 47.1096 - mse: 47.1096\n",
            "Epoch 194/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 42.4233 - mse: 42.4233\n",
            "Epoch 195/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 31.2127 - mse: 31.2127\n",
            "Epoch 196/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 32.4609 - mse: 32.4609\n",
            "Epoch 197/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 44.0471 - mse: 44.0471\n",
            "Epoch 198/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 47.1187 - mse: 47.1187\n",
            "Epoch 199/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 47.8988 - mse: 47.8988\n",
            "Epoch 200/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 45.8741 - mse: 45.8741\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "R2 score of Neural Network model is  0.524040054219596\n",
            "mse of Neural Network model is  20.538539956797738\n",
            "mae of Neural Network model is  3.6793989880879727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ptt_aGJrON5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}